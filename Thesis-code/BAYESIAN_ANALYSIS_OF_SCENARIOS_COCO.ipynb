{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPtx8wLmH0riCxBqD6e/V/w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RvWBggF7pkZi","executionInfo":{"status":"ok","timestamp":1711804398764,"user_tz":-60,"elapsed":24877,"user":{"displayName":"Sushant","userId":"01539954029328098546"}},"outputId":"398a8b7c-70de-4f9d-a461-c4e487b832f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Collecting rouge\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.1\n","Cloning into 'pycocoevalcap'...\n","remote: Enumerating objects: 821, done.\u001b[K\n","remote: Counting objects: 100% (24/24), done.\u001b[K\n","remote: Compressing objects: 100% (20/20), done.\u001b[K\n","remote: Total 821 (delta 5), reused 19 (delta 4), pack-reused 797\u001b[K\n","Receiving objects: 100% (821/821), 130.06 MiB | 23.51 MiB/s, done.\n","Resolving deltas: 100% (424/424), done.\n"]}],"source":["!pip install nltk rouge\n","!git clone https://github.com/salaniz/pycocoevalcap"]},{"cell_type":"code","source":["from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"id":"1bjPXRaVnlgq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711804424708,"user_tz":-60,"elapsed":25950,"user":{"displayName":"Sushant","userId":"01539954029328098546"}},"outputId":"815c2814-8a6e-4436-80a9-60ff6499c8c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import json\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from random import shuffle\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","def load_json(file_path):\n","    with open(file_path, 'r') as f:\n","        return json.load(f)\n","\n","def calculate_corpus_bleu(references, candidate):\n","    references_tokenized = [[word_tokenize(ref.lower()) for ref in references]]\n","    candidate_tokenized = [word_tokenize(candidate.lower())]\n","    smoothing_function = SmoothingFunction().method1\n","    return corpus_bleu(references_tokenized, candidate_tokenized, smoothing_function=smoothing_function)\n","\n","def calculate_cosine_similarity(text1, text2):\n","    vectorizer = TfidfVectorizer()\n","    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n","    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n","\n","def calculate_average_cosine_similarity(generated_caption, original_captions):\n","    similarities = [calculate_cosine_similarity(generated_caption, original) for original in original_captions]\n","    return sum(similarities) / len(similarities)\n","\n","# Function to calculate the average of a list\n","def avg(lst):\n","    return sum(lst) / len(lst) if lst else 0\n","\n","# Load the entire dataset\n","data = load_json('/content/drive/MyDrive/MasterThesis/Scenario1_distilbart_with_all_similarities_only_2.json')\n","\n","# Shuffle and split the dataset into training (80%) and testing (20%) segments\n","items = list(data.values())\n","shuffle(items)\n","split_index = int(0.8 * len(items))\n","data_train = items[:split_index]\n","data_test = items[split_index:]\n","\n","# Initialize counters for calculating priors based on cosine similarity\n","better_count_train = {'blip': 0, 'gpt2': 0}\n","\n","# Calculate priors based on average cosine similarity in training data\n","for item in data_train:\n","    original_captions = [' '.join(item['original_coco_captions'])]  # Combine all original captions into a single text for simplicity\n","    cos_similarities = {model: calculate_average_cosine_similarity(item['generated_captions'][model], original_captions) for model in ['gpt2', 'blip']}\n","    better_model = max(cos_similarities, key=cos_similarities.get)\n","    better_count_train[better_model] += 1\n","\n","total_instances_train = sum(better_count_train.values())\n","prior_gpt = better_count_train['gpt2'] / total_instances_train\n","prior_blip = better_count_train['blip'] / total_instances_train\n","\n","# Calculate likelihoods using testing data\n","bleu_scores_test = {'blip': [], 'gpt2': []}\n","for item in data_test:\n","    original_captions = item['original_coco_captions']\n","    for model in ['gpt2', 'blip']:\n","        bleu_score = calculate_corpus_bleu(original_captions, item['generated_captions'][model])\n","        bleu_scores_test[model].append(bleu_score)\n","\n","likelihood_gpt = avg(bleu_scores_test['gpt2'])\n","likelihood_blip = avg(bleu_scores_test['blip'])\n","\n","# Calculate Marginal Likelihood (Evidence)\n","marginal_likelihood = likelihood_gpt * prior_gpt + likelihood_blip * prior_blip\n","\n","# Calculate Posterior probabilities\n","posterior_gpt = (likelihood_gpt * prior_gpt) / marginal_likelihood\n","posterior_blip = (likelihood_blip * prior_blip) / marginal_likelihood\n","\n","print(f\"Prior for GPT: {prior_gpt:.4f}, Prior for BLIP: {prior_blip:.4f}\")\n","print(f\"Likelihood (Average BLEU) for GPT on test data: {likelihood_gpt:.4f}\")\n","print(f\"Likelihood (Average BLEU) for BLIP on test data: {likelihood_blip:.4f}\")\n","print(f\"Marginal Likelihood (Evidence): {marginal_likelihood:.4f}\")\n","print(f\"Posterior for GPT: {posterior_gpt:.4f}\")\n","print(f\"Posterior for BLIP: {posterior_blip:.4f}\")"],"metadata":{"id":"g0A5MtdVrXaz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711804434522,"user_tz":-60,"elapsed":9819,"user":{"displayName":"Sushant","userId":"01539954029328098546"}},"outputId":"38815141-9d1a-4c79-b833-b5947a381c77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Prior for GPT: 0.5482, Prior for BLIP: 0.4518\n","Likelihood (Average BLEU) for GPT on test data: 0.3202\n","Likelihood (Average BLEU) for BLIP on test data: 0.2964\n","Marginal Likelihood (Evidence): 0.3095\n","Posterior for GPT: 0.5672\n","Posterior for BLIP: 0.4328\n"]}]},{"cell_type":"code","source":["import json\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from random import shuffle\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","def load_json(file_path):\n","    with open(file_path, 'r') as f:\n","        return json.load(f)\n","\n","def calculate_corpus_bleu(references, candidate):\n","    references_tokenized = [[word_tokenize(ref.lower()) for ref in references]]\n","    candidate_tokenized = [word_tokenize(candidate.lower())]\n","    smoothing_function = SmoothingFunction().method1\n","    return corpus_bleu(references_tokenized, candidate_tokenized, smoothing_function=smoothing_function)\n","\n","def calculate_cosine_similarity(text1, text2):\n","    vectorizer = TfidfVectorizer()\n","    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n","    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n","\n","def calculate_average_cosine_similarity(generated_caption, original_captions):\n","    similarities = [calculate_cosine_similarity(generated_caption, original) for original in original_captions]\n","    return sum(similarities) / len(similarities)\n","\n","# Function to calculate the average of a list\n","def avg(lst):\n","    return sum(lst) / len(lst) if lst else 0\n","\n","# Load the entire dataset\n","data = load_json('/content/drive/MyDrive/MasterThesis/Scenario2_distilbart_with_all_similarities_only_2.json')\n","\n","# Shuffle and split the dataset into training (80%) and testing (20%) segments\n","items = list(data.values())\n","shuffle(items)\n","split_index = int(0.8 * len(items))\n","data_train = items[:split_index]\n","data_test = items[split_index:]\n","\n","# Initialize counters for calculating priors based on cosine similarity\n","better_count_train = {'blip': 0, 'gpt2': 0}\n","\n","# Calculate priors based on average cosine similarity in training data\n","for item in data_train:\n","    original_captions = [' '.join(item['original_coco_captions'])]  # Combine all original captions into a single text for simplicity\n","    cos_similarities = {model: calculate_average_cosine_similarity(item['generated_captions'][model], original_captions) for model in ['gpt2', 'blip']}\n","    better_model = max(cos_similarities, key=cos_similarities.get)\n","    better_count_train[better_model] += 1\n","\n","total_instances_train = sum(better_count_train.values())\n","prior_gpt = better_count_train['gpt2'] / total_instances_train\n","prior_blip = better_count_train['blip'] / total_instances_train\n","\n","# Calculate likelihoods using testing data\n","bleu_scores_test = {'blip': [], 'gpt2': []}\n","for item in data_test:\n","    original_captions = item['original_coco_captions']\n","    for model in ['gpt2', 'blip']:\n","        bleu_score = calculate_corpus_bleu(original_captions, item['generated_captions'][model])\n","        bleu_scores_test[model].append(bleu_score)\n","\n","likelihood_gpt = avg(bleu_scores_test['gpt2'])\n","likelihood_blip = avg(bleu_scores_test['blip'])\n","\n","# Calculate Marginal Likelihood (Evidence)\n","marginal_likelihood = likelihood_gpt * prior_gpt + likelihood_blip * prior_blip\n","\n","# Calculate Posterior probabilities\n","posterior_gpt = (likelihood_gpt * prior_gpt) / marginal_likelihood\n","posterior_blip = (likelihood_blip * prior_blip) / marginal_likelihood\n","\n","print(f\"Prior for GPT: {prior_gpt:.4f}, Prior for BLIP: {prior_blip:.4f}\")\n","print(f\"Likelihood (Average BLEU) for GPT on test data: {likelihood_gpt:.4f}\")\n","print(f\"Likelihood (Average BLEU) for BLIP on test data: {likelihood_blip:.4f}\")\n","print(f\"Marginal Likelihood (Evidence): {marginal_likelihood:.4f}\")\n","print(f\"Posterior for GPT: {posterior_gpt:.4f}\")\n","print(f\"Posterior for BLIP: {posterior_blip:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"130HlFQ_f-mD","executionInfo":{"status":"ok","timestamp":1711804523941,"user_tz":-60,"elapsed":9006,"user":{"displayName":"Sushant","userId":"01539954029328098546"}},"outputId":"8cba0ce2-739f-4d52-a481-94a8b0886d97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Prior for GPT: 0.5608, Prior for BLIP: 0.4392\n","Likelihood (Average BLEU) for GPT on test data: 0.3054\n","Likelihood (Average BLEU) for BLIP on test data: 0.3245\n","Marginal Likelihood (Evidence): 0.3138\n","Posterior for GPT: 0.5458\n","Posterior for BLIP: 0.4542\n"]}]}]}